{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mineptune/Text_Toxicity_Moderation/blob/main/Text_Toxicity_Moderation_Model_With_Gradio_FIXED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fcd3e0c",
      "metadata": {
        "id": "0fcd3e0c"
      },
      "source": [
        "# üß† Text Toxicity Moderation Model\n",
        "**Author:** [Your Name]  \n",
        "**Environment:** Jupyter Notebook  \n",
        "**Objective:** Detect whether a piece of text is toxic or non-toxic using NLP techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3dcaa1f",
      "metadata": {
        "id": "b3dcaa1f"
      },
      "source": [
        "## üß≠ 1. Introduction\n",
        "### Objective\n",
        "The goal of this notebook is to develop a model that can automatically identify toxic comments ‚Äî such as hate speech, insults, or harassment ‚Äî using natural language processing (NLP).\n",
        "\n",
        "### Why It Matters\n",
        "Text toxicity detection is critical for moderating online platforms, maintaining community standards, and reducing online abuse.\n",
        "\n",
        "### Approach\n",
        "We'll explore two approaches:\n",
        "1. **Baseline Model:** TF-IDF + Logistic Regression  \n",
        "2. **Advanced Model:** Pre-trained BERT (Hugging Face)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b85a1df",
      "metadata": {
        "id": "0b85a1df"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è Setup and Imports\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn transformers torch gradio SpeechRecognition --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af9320e",
      "metadata": {
        "id": "5af9320e"
      },
      "outputs": [],
      "source": [
        "# üì• Load the Dataset\n",
        "data = {\n",
        "    'text': [\n",
        "        \"I love this product!\",\n",
        "        \"You are such an idiot.\",\n",
        "        \"What a wonderful day.\",\n",
        "        \"This is the worst thing ever.\",\n",
        "        \"I hate you so much.\",\n",
        "        \"Great job on the project!\"\n",
        "    ],\n",
        "    'label': [0, 1, 0, 1, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e71a6e",
      "metadata": {
        "id": "b7e71a6e"
      },
      "outputs": [],
      "source": [
        "# üîç Exploratory Data Analysis (EDA)\n",
        "print(df['label'].value_counts())\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.title(\"Distribution of Toxic vs Non-Toxic Comments\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd27fcc",
      "metadata": {
        "id": "7bd27fcc"
      },
      "outputs": [],
      "source": [
        "# üßπ Data Preprocessing\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33af5d8d",
      "metadata": {
        "id": "33af5d8d"
      },
      "outputs": [],
      "source": [
        "# üß© Baseline Model ‚Äì TF-IDF + Logistic Regression\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vec, y_train)\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc1720d",
      "metadata": {
        "id": "ecc1720d"
      },
      "outputs": [],
      "source": [
        "# ü§ñ Advanced Model ‚Äì Pre-trained BERT (Hugging Face)\n",
        "from transformers import pipeline\n",
        "\n",
        "toxic_model = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
        "\n",
        "examples = [\n",
        "    \"I hate this product so much!\",\n",
        "    \"You're amazing and kind.\",\n",
        "    \"Go die in a hole.\"\n",
        "]\n",
        "for e in examples:\n",
        "    print(e, \"‚Üí\", toxic_model(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a817f73",
      "metadata": {
        "id": "4a817f73"
      },
      "outputs": [],
      "source": [
        "# üìä Model Evaluation and Examples\n",
        "def predict_toxicity(text):\n",
        "    cleaned = clean_text(text)\n",
        "    vec = vectorizer.transform([cleaned])\n",
        "    pred = model.predict(vec)[0]\n",
        "    return \"Toxic\" if pred == 1 else \"Non-Toxic\"\n",
        "\n",
        "sample_texts = [\n",
        "    \"I can't stand you.\",\n",
        "    \"You're doing great work.\",\n",
        "    \"This app sucks.\"\n",
        "]\n",
        "\n",
        "for t in sample_texts:\n",
        "    print(f\"{t} ‚Üí {predict_toxicity(t)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6192ec9",
      "metadata": {
        "id": "a6192ec9"
      },
      "source": [
        "## üí° Conclusion & Future Work\n",
        "### Results Summary\n",
        "- Logistic Regression (TF-IDF) achieved decent baseline performance.\n",
        "- BERT-based model (pretrained) delivers higher contextual accuracy.\n",
        "\n",
        "### Future Improvements\n",
        "- Fine-tune BERT on a custom dataset.\n",
        "- Add explainability (e.g., SHAP values).\n",
        "- Integrate into an API for real-time moderation.\n",
        "\n",
        "### Ethical Considerations\n",
        "- Ensure fairness: avoid bias against dialects, slang, or specific groups.\n",
        "- Consider privacy, transparency, and accountability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e6b9f4",
      "metadata": {
        "id": "98e6b9f4"
      },
      "outputs": [],
      "source": [
        "# üéôÔ∏è Interactive Gradio Interface (Text + Voice)\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import speech_recognition as sr\n",
        "\n",
        "# Load pre-trained model\n",
        "toxic_model = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
        "\n",
        "def classify_toxicity(text, audio=None):\n",
        "    # If audio provided, use speech recognition to extract text\n",
        "    if audio is not None:\n",
        "        try:\n",
        "            recognizer = sr.Recognizer()\n",
        "            with sr.AudioFile(audio) as source:\n",
        "                audio_data = recognizer.record(source)\n",
        "                text_from_audio = recognizer.recognize_google(audio_data)\n",
        "                text = text_from_audio\n",
        "        except Exception as e:\n",
        "            return f\"‚ö†Ô∏è Error recognizing speech: {e}\"\n",
        "    if not text or text.strip() == \"\":\n",
        "        return \"‚ö†Ô∏è Please enter or speak a sentence.\"\n",
        "    result = toxic_model(text)[0]\n",
        "    label = result['label']\n",
        "    score = round(result['score'], 3)\n",
        "    return f\"üßæ Input: {text}\\n\\nPrediction: {label}\\nConfidence: {score}\"\n",
        "\n",
        "# Launch Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=classify_toxicity,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Type a sentence here\"),\n",
        "        gr.Audio(source=\"microphone\", type=\"filepath\", label=\"üé§ Or speak your sentence (speech-to-text)\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"üß† Real-Time Text Toxicity Detector\",\n",
        "    description=\"Type or speak a sentence and see if it's toxic using a fine-tuned NLP model.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}